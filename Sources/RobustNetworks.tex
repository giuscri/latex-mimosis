%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Robust networks}
\label{ch:robust-networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Before running experiments on how ML models can be \emph{shielded}
against adversarial examples we had to create and train them.

\section{Motivation}
\label{sec:motivation}
As we're training a variety of models it's useful to find a strategy to
find an appropriate number of epochs to train the model for. Fixing a
number of epochs (say 500) and train all the models for that same
number of epochs feels \emph{cheesy}: some models can learn faster,
other slower, and you can end up comparing models that are at different
stage in their learning phase.

What we did was trying to \emph{understand} when the model has done
learning and stop training there. We used two approaches/heuristics
that are described in the following sections.

Note that trying to make the model learn better we reduced the learning
rate on plateau: if for a fixed number of epochs the accuracy on the
validation set does no longer improve, the learning rate is reduced.
This is known to help models to move on when the gradient descent
algorithm is stuck somewhere.

To make our experiments here we used a fully-connected neural network
with 2 layers of 100 hidden neurons each and 10 output neurons --
\emph{FC-100-100-10}. This structure is the same described in ???,
which is the paper we got inspiration from. ??? trained FC-100-100-10
for 500 epochs and we assume they chose that number as it gave them the
best results on the test set.

What we want to do is to empirically prove that we can achieve the
same accuracy on the test set without training FC-100-100-10 for
exactly 500 epochs but instead stopping training when the model stops
learning according to at least one of our heuristics.

\section{EarlyStopping}
\emph{EarlyStopping} is one of the two heuristics. It's already
implemented by Keras as a Callback. It checks if over a specified
number of epochs (the \emph{patience}) a specified metrics stopped
improving. When that happens the model is asked to stop training there.
We set a \emph{patience} of 60 epochs as on Google Cloud Platform 1
epochs runs is ~1 second; a patience of 60 epochs means that the model
didn't improve for a whole minute.

\section{StopOnStableWeights}
\emph{StopOnStableWeights} is the second heuristic. It has to be
implemented from scratch. This heuristic collects data on model's
weights over a specified number of epochs called the \emph{patience}.
Then, the same weight over different epochs is checked: if it has a
small standard deviation maybe it's getting stable. If the maximum of
these standard deviation is below a certain threshold we conclude the
model's weights are getting stable, hence the model is stopping
learning.

\section{Data obtained}
We obtained the following data:
\begin{itemize}
\item
  Model trained for 1000 epochs: accuracy on the test set of 97.71\%
\item
  Model trained for 500 epochs: accuracy on the test set of 97.55\%
\item
  Model trained for 500 epochs, reducing learning rate on plateau after
  30 epochs: accuracy on the test set of 97.47\%
\item
  Model trained for -1 epochs, stopping after accuracy on the
  validation set stopped improving for 60 epochs: accuracy on the test
  set of 97.44\% (trained for 424 epochs)
\item
  Model trained for -1 epochs, stopping as model weights had a maximum
  standard deviation of 0.5 over 60 epochs: accuracy on the test set of
  97.66\% (trained for 614 epochs)
\item
  Model trained for -1 epochs, stopping after accuracy on the
  validation set stopped improving for 60 epochs, reducing learning
  rate on plateau after 30 epochs: accuracy on the test
  set of 97.18\% (trained for 510 epochs)
\item
  Model trained for -1 epochs, stopping as model weights had a maximum
  standard deviation of 0.5 over 60 epochs, reducing learning
  rate on plateau after 30 epochs: accuracy on the test set of
  97.6\% (trained for 514 epochs)
\end{itemize}

\section{Conclusion}
From the above results the best approach seems to be using the
\emph{StopOnStableWeights} heuristics with a patience of 60 epochs,
reducing the learning rate on plateau after 30 epochs.

In the following chapters we'll use this setting for training models,
even when they're no longer the FC-100-100-10 used here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Reconstruction or retraining?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As we're testing different filter techniques it's important that we
understand how we're implementing these techniques in our models as
defenses.

\section{Motivation}
\label{sec:motivation}
There are at least two different fashions to integrate a
filter/decomposition technique in our models. We can either add a layer
to a pre-trained model and use it only \emph{after} training, or we can
add a layer to a pre-trained model and re-train the network after this
layer has been added. In the latter case, the model has been trained
only on filtered input, while in the former it does not. Following
lexicon used in ???, we named the first technique as
\emph{reconstruction} and the second one \emph{retraining}.

There's another alternative which is to initialize the model, add the
filter layer then train the network while it's still freshly
initialized. It basically consists in making the network never to see
unfiltered input. This seemed to make things so much better for the
attacker than both reconstruction and retraining that we just avoided
to compare it with the other approaches. Instead when it comes to
reconstruction and retraining who is better is not that clear. We did
some measurements to better understand it.

\section{How did we perform the comparison}

We built and train models using both reconstruction and retraining. We
had now two families of models. For example, for FC-100-100-10 with a
filter layer implementing PCA retaining 80 components we had one model
trained using reconstruction and another trained using retraining.

We attacked each model using Fast Gradient Sign obtaining a curve
saying what was the adversarial success score as $\eta$ increases --
where $\eta$ is the freedom given to the attacker: the more the freedom
the easier is to forge an input. Then, we paired each model trained
using reconstruction with the same model trained using retraining. We
computed the average value of the curve obtained subtracting the curve
for retraining from the curve for reconstruction. At last we computed
the average for the set of pairs, getting what is the average difference
between reconstruction and retraining.

\section{Data obtained}
We obtained that retraining is only 1\% more effective than reconstruction
when it comes to protect models from adversarial input.

\section{Conclusion}
While implementing retraining defense is not hard we concluded the
difference is not significant enough and decided to stick with
reconstruction which is a clearer solution -- it allows us to think about
filter techiniques without worrying about how that impacts on training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\chapter{Image filters as a defense against adversarial input}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter we'll test a couple of image filters against
adversarial input. The intuition behind the idea of filtering the image
is that to forge an adversarial input the attacker will try to put
little noise distributed in the image. As these filters highlight the
\emph{important} features of an image we hope they cut out the noise
introduced by the attacker.

For each of the filters tested in this chapter we can set its
parameters such that it can be more or less destructive in regards of
the original information. Intuitively the more invasive the filter the
better will be the defense. Unfortunately the better the defense the
more the lost accuracy too: two different images can be confused as the
same image for the model as the information that distiguished the two
images is now potentially lost. That means that the \emph{best} filter
will be the one that provides the best defense given the accuracy lost
by the model is not \emph{too much}.

<<SHOULDNT WE WRITE AN ATTACK CHAPTER?>> As a baseline we performed an
attack of Fast Gradient Sign with its parameter $\eta$ set to 0.1. We
chose that value for $\eta$ as it's a median value between a
non-effective attack -- when $\eta$ equals 0 -- and an attack that's
mostly unstoppable -- when $\eta$ is greater than 0.25.

<<INSERT 3 IMAGES OF THE SAME INPUT AS ETA INCREASES>>

In our \emph{gray-box} setting we measured an accuracy on the MNIST
test set of 97.39\% and a probability of success for the attacker of
81\%. Again, we expected these quantities to go down as we increase the
\emph{power} of the filter technique: while we try to stop the attacker
we have to reduce the accuracy of the model too.

\section{Principal Component Analysis}

<<WRITE GENERAL STUFF ON PCA>>

We tried the following setting for PCA: retain all the components of an
image (784 pixels), 331 of them, 100 of them, 20 and 10 components.

\begin{itemize}
  \item Retaining all the 784 components: accuracy 97.38\%, adversarial
    success score 82\%.
  \item Retaining 331 principal components: accuracy 97.52\%,
    adversarial success score 71\%.
  \item Retaining 100 principal components: accuracy 97.48\%,
    adversarial success score 44\%.
  \item Retaining 20 principal components: accuracy 94.18\%,
    adversarial success score 23\%.
  \item Retaining 10 principal components: accuracy 83\%, adversarial
    success score 22\%.
\end{itemize}

Of all these setting the best result is achieved when we retained 100
components. The accuracy on the MNIST test set is pretty the same (in
fact, it's even increased with the help of the filter) and the
adversarial success score is halved: from the original probability of
81\% to 44\%. When we're comparing PCA with other image filters we'll
consider this setting of 100 components.
