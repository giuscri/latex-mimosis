%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Tools and libraries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter I describe the tools and libraries that I used to
implement the set of experiments described in Chapter
\ref{ch:robust-networks}. This chapter is organized as follows: Section
\ref{sec:tensorflow} is dedicated to
TensorFlow\footnote{https://www.tensorflow.org/} and its computational
graph; Section \ref{sec:keras} is for Keras, a high-level API for
TensorFlow; Section \ref{sec:cleverhans} describes CleverHans, a
library to generate adversarial examples; Section \ref{sec:sklearn}
talks about scikit-learn, a library for building and training Machine
Learning models while Section \ref{sec:google-cloud-platform} briefly
mentions Google Cloud Platform, a service by Google that allows you to
buy computational power to run your own code.

\section{TensorFlow}
\label{sec:tensorflow}

\begin{figure}
  \begin{minted}{python}
    >>> import tensorflow as tf
    >>> symbol = tf.constant(42)
    >>> symbol
    <tf.Tensor 'Const:0' shape=() dtype=int32>
  \end{minted}
  \caption{Building a \emph{constant} tensor}
  \label{fig:fortytwo}
\end{figure}

TensorFlow is a C++ framework for Machine Learning released by Google.
It uses a peculiar programming model called Data Flow. While that
allows parallel and distributed computation, it can be quite daunting
to use when tinkering. In fact, operations to perform on data are first
described and only later executed. I believe this can be
counter-intuitive for most people. For example, in Figure
\ref{fig:fortytwo} \texttt{symbol} doesn't contain a reference to the
integer 42 --- as opposed to what would have been if \texttt{symbol} was
an \texttt{int} variable. Instead it contains a reference to a node of
the \emph{computational graph} (a description of the computation to
perform on data in terms of nodes and edges) which will always output
42. In TensorFlow computation is done by connecting one or more of
these nodes to other nodes, letting \emph{tensors} (``the main object
you manipulate and pass around [in TensorFlow]''%
\footnote{https://web.archive.org/web/20180921135324/https://www.tensorflow.org/guide/tensors})
move around this graph (in case of Figure \ref{fig:fortytwo}, the
tensor is the output of the only one node in the graph: the tensor is
42). Problem is that in general until you don't run the computational
graph it's hard to determine what's going to be the value of a tensor,
that is the \emph{output} of the computational graph. This is slowing
down exploratory analysis.

\subsection{About the computational graph and TensorFlow's Python API}
\label{subsec:computational-graph}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=latex,line join=bevel,]
  \node (a) at (27.0bp,72.0bp) [draw,ellipse] {a}; \node
  (add) at (117.95bp,45.0bp) [draw,ellipse] {add}; \node (b)
  at (27.0bp,18.0bp) [draw,ellipse] {b}; \draw [->] (b)
  ..controls (61.417bp,28.218bp) and (72.527bp,31.516bp) ..
  (add); \draw [->] (a) ..controls (61.417bp,61.782bp) and
  (72.527bp,58.484bp) .. (add);
  \end{tikzpicture}
  \caption[]{Crazy simple computational graph}
  \label{fig:easy-graph}
\end{figure}

TensorFlow's computational graph is a fundamental part of the
framework. A computational graph is a directed graph representing a
computation involving tensors. A tensor in TensorFlow is a
multi-dimensional array: it can be a scalar, a matrix, a batch of RGB
images (which is a 4D vector\footnote{If you always thought human can't
  visualize four dimensions all at once, think again.}), etc. Each node
represents an operation on tensors, while edges represent
tensors.\footnote{This distinction between nodes and edges is important
  more from a formal standpoint than a practical one: actually thinking
  of tensors as nodes makes no difference to the best of my knowledge.}

For example, Figure \ref{fig:easy-graph} shows a very simple
computational graph. It's the one associated to an addition of two
\texttt{tf.float32}'s (i.e. two tensors made of 32-bit float numbers):
the first two nodes \texttt{a} and \texttt{b} output one tensor each;
those are used to feed the \texttt{add} node that will output the
tensor resulting from the sum of \texttt{a} and \texttt{b}.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=latex,line join=bevel,]
    \node (s3) at (27.0bp,90.0bp) [draw,ellipse] {}; \node
    (s2) at (243.0bp,90.0bp) [draw,ellipse] {}; \node (s1)
    at (171.0bp,90.0bp) [draw,ellipse] {}; \node (s4) at
    (99.0bp,90.0bp) [draw,ellipse] {}; \node (d4) at
    (99.0bp,18.0bp) [draw,ellipse] {}; \node (d2) at
    (243.0bp,18.0bp) [draw,ellipse] {}; \node (d3) at
    (27.0bp,18.0bp) [draw,ellipse] {}; \node (d1) at
    (171.0bp,18.0bp) [draw,ellipse] {}; \draw [->] (s1)
    ..controls (171.0bp,64.131bp) and (171.0bp,54.974bp) ..
    (d1); \draw [->] (s2) ..controls (196.78bp,66.889bp) and
    (157.24bp,47.119bp) .. (d4); \draw [->] (s3) ..controls
    (89.947bp,69.018bp) and (165.24bp,43.919bp) .. (d2);
    \draw [->] (s1) ..controls (145.8bp,64.803bp) and
    (132.68bp,51.685bp) .. (d4); \draw [->] (s4) ..controls
    (124.2bp,64.803bp) and (137.32bp,51.685bp) .. (d1);
    \draw [->] (s2) ..controls (243.0bp,64.131bp) and
    (243.0bp,54.974bp) .. (d2); \draw [->] (s3) ..controls
    (52.197bp,64.803bp) and (65.315bp,51.685bp) .. (d4);
    \draw [->] (s4) ..controls (145.22bp,66.889bp) and
    (184.76bp,47.119bp) .. (d2); \draw [->] (s2) ..controls
    (180.05bp,69.018bp) and (104.76bp,43.919bp) .. (d3);
    \draw [->] (s4) ..controls (73.803bp,64.803bp) and
    (60.685bp,51.685bp) .. (d3); \draw [->] (s3) ..controls
    (27.0bp,64.131bp) and (27.0bp,54.974bp) .. (d3); \draw
          [->] (s3) ..controls (73.222bp,66.889bp) and
          (112.76bp,47.119bp) .. (d1); \draw [->] (s2)
          ..controls (217.8bp,64.803bp) and
          (204.68bp,51.685bp) .. (d1); \draw [->] (s1)
          ..controls (124.78bp,66.889bp) and
          (85.238bp,47.119bp) .. (d3); \draw [->] (s1)
          ..controls (196.2bp,64.803bp) and
          (209.32bp,51.685bp) .. (d2); \draw [->] (s4)
          ..controls (99.0bp,64.131bp) and (99.0bp,54.974bp)
          .. (d4);
  \end{tikzpicture}
  \caption[]{Our feedforward neural network}
  \label{fig:neural-network}
\end{figure}

The most commonly used way to program the computational graph is by
using a Python API. A perhaps interesting example of the kind of
computation one can express in TensorFlow using Python would the
implementation of a neural network. Figure \ref{fig:neural-network}
graphically represents the network we're going to implement: a simple
feedforward
network\footnote{https://en.wikipedia.org/wiki/Feedforward\_neural\_network}
of four input neurons and four output neurons. As explained in Chapter
\ref{ch:background} what a feedforward neural network does is a linear
operation on its input and the application of an \emph{activation
  function} on that result. That is, it computes

\[ \text{activation}(X * W + b) \]

given its weight matrix and its bias vector, \( W \in \mathbb{R}^{4
  \times 4} \) and \( b \in \mathbb{R}^{4}\) respectively, while
\( \text{activation} \) is a \emph{non}-linear function such as
ReLU\footnote{https://en.wikipedia.org/wiki/Rectifier\_(neural\_networks)
}.

\begin{figure}
  \begin{minted}{python}
    >>> import tensorflow as tf
    >>> W = tf.random_normal(shape=(4, 4))
    >>> b = tf.random_normal(shape=(4,))
    >>> X = tf.placeholder(shape=(None, 4), dtype=tf.float32)
    >>> logits = tf.matmul(X, W) + b
    >>> probabilities = tf.nn.softmax(logits)
    >>> probabilities
    <tf.Tensor 'Softmax:0' shape=(?, 4) dtype=float32>
  \end{minted}
  \caption{TensorFlow commands to generate a feedforward network}
  \label{fig:tensorflow-feedforward}
\end{figure}

To get an idea of how to do that in TensorFlow see Figure
\ref{fig:tensorflow-feedforward}. Note that TensorFlow already
implements the activation function \texttt{tf.nn.softmax}. That
function is applied to the result of the matrix multiplication. As
explained in Chapter \ref{ch:background} this is done for various
reasons: it introduces a \emph{non-linear factor} in the network
computations allowing it to learn how to compute a larger set of
functions\footnote{https://en.wikipedia.org/wiki/Universal\_approximation\_theorem};
also it squashes the output of the network in $[0,1]$ allowing to
interpret it in terms of \emph{confidence levels}.

\begin{figure}
  \begin{minted}{python}
    >>> import numpy as np
    >>> batch = np.random.rand(10, 4)
    >>> batch
    >>> array([[0.54485176, 0.33854871, 0.45185129, 0.79884188],
      [0.41204776, 0.23552753, 0.04101023, 0.47883844],
      [0.25544491, 0.7610509 , 0.49307137, 0.6098213 ],
      [0.02545156, 0.70459456, 0.22067103, 0.64743811],
      [0.92359354, 0.96497353, 0.45790538, 0.49380769],
      [0.13330072, 0.22947966, 0.02996348, 0.69954114],
      [0.38397249, 0.30473362, 0.87023559, 0.90153084],
      [0.77056319, 0.94843128, 0.39095345, 0.50572861],
      [0.90112077, 0.19240995, 0.48437166, 0.46200152],
      [0.98589042, 0.2013479 , 0.86091217, 0.55886214]])
    >>> with tf.Session() as session:
    ...     session.run(probabilities, feed_dict={X: batch})
    ...
    >>> array([[0.03751625, 0.8651346 , 0.05245555, 0.04489357],
      [0.11873736, 0.6301607 , 0.17646323, 0.07463871],
      [0.06252376, 0.82338244, 0.07284217, 0.04125158],
      [0.12086256, 0.6911012 , 0.14362463, 0.04441159],
      [0.03662292, 0.8575108 , 0.04834893, 0.05751737],
      [0.12984137, 0.63064647, 0.1834405 , 0.05607158],
      [0.01711418, 0.93650085, 0.02116078, 0.02522417],
      [0.04886489, 0.83023334, 0.06332602, 0.05757581],
      [0.033136 , 0.84808177, 0.05061754, 0.06816475],
      [0.01250888, 0.9262628 , 0.01797607, 0.0432523 ]], dtype=float32)
  \end{minted}
  \caption{Running a computational graph within a session}
  \label{fig:use-session}
\end{figure}

\begin{figure}
  \begin{minted}{python}
    >>> W = tf.Variable(W)
    >>> b = tf.Variable(b)
    >>> logits = tf.matmul(X, W) + b # redefine logits using variables 
    >>> probabilities = tf.nn.softmax(logits)
  \end{minted}
  \caption{Making variables out of \texttt{W} and \texttt{b}}
  \label{fig:making-variables}
\end{figure}

Now, to get results out of the graph you need what's called a
\emph{session} in TensorFlow. Basically a session \emph{powers on} the
graph, allowing you to run the computational graph with your own data
and get actual tensors of actual numbers out of it. Of course building
neural networks is pretty useless if you can't train them: at first the
whole output is only based on the random weights and biases randomly
extracted from a uniform distribution --- for example, probabilities in
Figure \ref{fig:use-session} are totally non-sense. There's no training
set and no training phase yet. To train a network in TensorFlow we need
a \texttt{tf.Variable}. A variable in TensorFlow is a tensor whose
value persists across sessions and that session can modify. This allows
you to basically add parameters to your computational graphs, allowing
to iteratively change the function computed on the graph. That,
combined with a learning procedure that modifies the
\texttt{tf.Variable}s to reduce the distance from the target function
to the actual function computed by the network, make the model
\emph{learning}. In the current example, the part of the graph that you
want the learning procedure to modify are $W$ and $b$. As in Figure
\ref{fig:making-variables} to build a \texttt{tf.Variable} in
TensorFlow you can just wrap an existing tensor.

\begin{figure}
  \begin{minted}{python}
    >>> y_true = tf.placeholder(shape=(None,), dtype=tf.float32)
    >>> cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_true)
  \end{minted}
  \caption{Creating the cross-entropy operation}
  \label{fig:cross-entropy}
\end{figure}

\begin{figure}
  \begin{minted}[linenos]{python}
    from tf.train import GradientDescentOptimizer as SGD
    optimizer = SGD(learning_rate=0.5).minimize(cross_entropy)

    with tf.Session() as session:
        session.run(tf.global_variables_initializer()) # needed.

    n_training_steps = 10
    for i in range(n_training_steps):
        images, classes = next_batch()

        with tf.Session() as session:
            session.run(optimizer, feed_dict={X: images, y_true: classes})
  \end{minted}
  \caption{Training a feedforward neural network built with TensorFlow}
  \label{fig:training-network}
\end{figure}

The last thing we want to do before abandoning this example is the
actual training of the network. To perform training we need a training
set. As explained in Chapter \ref{ch:background}, a training set
consist of a batch of data and a label associated to each input,
representing the class of that data. As \emph{training} basically means
minimizing a loss function between the provided labels of the training
set and the labels guessed by the neural network, we need a loss
function: as in Figure \ref{fig:cross-entropy}, we're using
\texttt{tf.nn.softmax\_cross\_entropy\_with\_logits\_v2}\footnote{https://en.wikipedia.org/wiki/Cross\_entropy\#Cross-entropy\_error\_function\_and\_logistic\_regression}.
The gradient descent
algorithm\footnote{https://en.wikipedia.org/wiki/Gradient\_descent} is
then used to iteratively minimize the cross-entropy computed against
the network output. TensorFlow implements that algorithm via
\texttt{GradientDescentOptimizer.minimize()} that returns an operation
that each time you run in a session will modify the graph's
\texttt{tf.Variable}s, in the quest of lowering the value of the loss
function. We can build a loop to train the network over and over, as in
Figure \ref{fig:training-network}. This will push the loss function
toward a local minimum; hopefully a useful one, i.e. a minimum that
corresponds to good results even on data outside the training set.

\section{Keras}
\label{sec:keras}

\begin{figure}
  \begin{minted}[linenos]{python}
    from keras.engine.base_layer import Layer
    import tensorflow as tf

    class Add42(Layer):
        def __init__(self):
            self.fortytwo = tf.constant(42)

        def __call__(self, X):
            return tf.add(X, self.fortytwo)

    model = Sequential([Dense(...), Add42(), ...])
  \end{minted}
  \caption{A toy-example for a Keras layer}
  \label{fig:toy-layer}
\end{figure}

Keras\footnote{https://keras.io/} is a library for building Machine
Learning models using a simple API that abstracts away the interface of
a backend of choice, e.g. TensorFlow. When you use the library, you're
encouraged to manipulate \emph{layers} instead of plain tensors. In
fact the whole concept of \emph{tensors} is hidden away by the library.
To implement your model you're expected to stack layers one on top the
other, expressing a \emph{sequential} manipulation of the
data\footnote{https://keras.io/\#getting-started-30-seconds-to-keras}.
Under the hood, a \texttt{Layer} is simply a callable object whose
\texttt{\_\_call()\_\_} method takes a TensorFlow tensor \texttt{X} and
manipulates it --- see Figure \ref{fig:toy-layer}.

\begin{figure}
  \begin{minted}{python}
    >>> from keras.models import Sequential
    >>> model = Sequential([
    ... Dense(batch_input_shape=(None, 4), units=4),
    ... Activation('softmax')])
  \end{minted}
  \caption{Feedforward network using Keras layers}
  \label{fig:feedforward-with-layers}
\end{figure}

As shown in Figure \ref{fig:feedforward-with-layers}, to implement the
simple feedforward neural network described in \ref{sec:tensorflow} one
would stack a \texttt{Dense} layer of 4 hidden neurons and an
\texttt{Activation} layer implementing the softmax function. Compare
that to the TensorFlow implementation and you'll see it's much more
easier to read and to think about. This nicer API that allows
to immediately think about model's architecture is paid an abstraction
level that at the time of this writing in my opinion is not enough
stable to allow programmers to be completely oblivious of the original
backend --- in my case TensorFlow.

\section{CleverHans}
\label{sec:cleverhans}

CleverHans\footnote{https://www.cleverhans.io/} is a library for
building adversarial examples written by Google, OpenAI and
Pennsylvania State University. It implements a variety of
attacks\footnote{https://cleverhans.readthedocs.io/en/latest/source/attacks.html}
against neural networks and it's compatible with models built with
Keras or just plain TensorFlow. It uses the same computational graphs
of TensorFlow to generate adversarial examples, so again using the
library knowing something about computational graphs helps.

\begin{figure}
  \begin{minted}[linenos]{python}
  from cleverhans.attacks import FastGradientMethod
  from cleverhans.utils_keras import KerasModelWrapper

  # `model` is a Keras model
  cleverhans_model = KerasModelWrapper(model)
  attack = FastGradientMethod(cleverhans_model)

  example_sym = attack.generate(model.input, **kwargs)
  # example_sym will give the adversarial examples
  # when run in a session.
  \end{minted}
  \caption{Generating adversarial examples using CleverHans}
  \label{fig:cleverhans-attack-dot-generate}
\end{figure}

To generate adversarial examples for a given model CleverHans needs to
be able to read some internals of the model --- inputs are generated in
a white-box approach. CleverHans already provides a couple of utilities
to do that \emph{model inspection} for some commonly used libraries ---
e.g. for Keras there's a
\texttt{KerasModelWrapper}\footnote{https://github.com/tensorflow/cleverhans/blob/66125be4e0e98686c5e005677263e52d3f3cea47/cleverhans/utils\_keras.py\#L101}
--- transforming the model into an object that CleverHans is able to
handle. Once a model has the interface CleverHans expects, it's
possible to choose the attack technique via classes inhereting from the
same \texttt{Attack} class. They are all exposing a \texttt{generate}
method that will return a node of the corresponding computational
graph; when you run it in a session it will return the related
adversarial examples. See Figure
\ref{fig:cleverhans-attack-dot-generate} for a short example.

\section{Scikit-learn}
\label{sec:sklearn}

Scikit-learn is a Python library written by David Cournapeau
providing a number of models, learning algorithms and other
utilities for Machine Learning. It's perfect for fast
prototyping as it uses a simple and consistent API and
handles numpy arrays or even Python lists.

I've used scikit-learn to borrow a couple of decomposition
algorithms (PCA, FastICA, ...) without having to implement
them. This required a bit of reasoning as reusing
scikit-learn algorithms on TensorFlow graphs was not
straightforward.

\section{Google Cloud Platform}
\label{sec:google-cloud-platform}

Google Cloud Platform is a set of services that allows
people to buy computational power from Google. In our case,
we used Google Cloud Platform to get access to a GPU for few
dollars a month. That made running experiments a lot faster
in many cases.
