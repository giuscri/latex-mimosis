%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Tools and libraries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter I describe the tools and libraries that I used to implement
the set experiments described in Chapter \ref{ch:tools-and-libraries}. This
chapter is organized as follows: Section \ref{sec:tensorflow} is dedicated to
TensorFlow \footnote{https://www.tensorflow.org/} and how its computational graphs work;
Section \ref{sec:keras} to Keras, a high-level API for building
Machine Learning models without working with low-level libraries like
TensorFlow; Section \ref{sec:cleverhans} to CleverHans, a
library to generate adversarial examples against a given model;
Section \ref{sec:sklearn} to scikit-learn, a library for building
and training Machine Learning models;
finally Section \ref{sec:google-cloud-platform} is about Google Cloud
Platform, a service by Google that allows you to buy computational
powers to run your own code.

\section{TensorFlow}
\label{sec:tensorflow}

TensorFlow is a C++ framework for Machine Learning released by Google.
It uses a programming model called Data Flow that aims to allow
distributed and parallel computations. While this \emph{paradigm} makes
TensorFlow suitable even for research and production environments, it
can be quite daunting to use when tinkering. In fact, computational
graphs are built and only later executed. This is counter-intuitive at
first.

\begin{figure}
  \begin{minted}{python}
    >>> import tensorflow as tf
    >>> symbol = tf.constant(42)
    >>> symbol
    <tf.Tensor 'Const:0' shape=() dtype=int32>
  \end{minted}
  \caption{}
  \label{fig:fortytwo}
\end{figure}

For example, in Figure \ref{fig:fortytwo}
\texttt{symbol} doesn't contain a reference to the integer 42. Instead
it contains a reference to a node of the computational graph (which
will always output 42). In general until you don't run the
computational graph it's hard to determine what's going to be the value
of a tensor (``the main object you manipulate and pass around [in TensorFlow]''%
\footnote{https://web.archive.org/web/20180921135324/https://www.tensorflow.org/guide/tensors}
-- see below). That's slowing down exploratory analysis.

\subsection{About computational graphs}
\label{subsec:computational-graph}

TensorFlow computational graphs are a fundamental part of the
framework. A computational graph is a directed graph representing a
computation involving tensors. A tensor in TensorFlow is a
multi-dimensional array: it can be a scalar, a matrix, a batch of RGB
images (which is a 4D vector), ... Each node represents an operation on
tensors, while edges represent tensors. (This distinction between nodes
and edges is more important from a formal standpoint than a practical
one: actually thinking of tensors as nodes makes no difference to the
best of my knowledge.)

Figure \ref{fig:easy-graph} shows a very simple computational graph.
It's the one associated to an addition of two \texttt{tf.float32}'s
(i.e. two tensors made of 32-bit float numbers). The first two nodes
\texttt{a} and \texttt{b} output a tensor each. Those are used to feed
the \texttt{add} node that will output the tensor resulting from the
sum of \texttt{a} and \texttt{b}.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=latex,line join=bevel,]
  \node (a) at (27.0bp,72.0bp) [draw,ellipse] {a};
    \node (add) at (117.95bp,45.0bp) [draw,ellipse] {add};
    \node (b) at (27.0bp,18.0bp) [draw,ellipse] {b};
    \draw [->] (b) ..controls (61.417bp,28.218bp) and (72.527bp,31.516bp)  .. (add);
    \draw [->] (a) ..controls (61.417bp,61.782bp) and (72.527bp,58.484bp)  .. (add);
  \end{tikzpicture}
  \caption[easy-graph]{An easy computational graph}
  \label{fig:easy-graph}
\end{figure}

A more interesting example would be to implement a neural network using
a computational graph. For example, Figure \ref{fig:neural-network}
graphically represents a simple neural network of four input neurons
and four output neurons.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=latex,line join=bevel,]
    \node (s3) at (27.0bp,90.0bp) [draw,ellipse] {};
    \node (s2) at (243.0bp,90.0bp) [draw,ellipse] {};
    \node (s1) at (171.0bp,90.0bp) [draw,ellipse] {};
    \node (s4) at (99.0bp,90.0bp) [draw,ellipse] {};
    \node (d4) at (99.0bp,18.0bp) [draw,ellipse] {};
    \node (d2) at (243.0bp,18.0bp) [draw,ellipse] {};
    \node (d3) at (27.0bp,18.0bp) [draw,ellipse] {};
    \node (d1) at (171.0bp,18.0bp) [draw,ellipse] {};
    \draw [->] (s1) ..controls (171.0bp,64.131bp) and (171.0bp,54.974bp)  .. (d1);
    \draw [->] (s2) ..controls (196.78bp,66.889bp) and (157.24bp,47.119bp)  .. (d4);
    \draw [->] (s3) ..controls (89.947bp,69.018bp) and (165.24bp,43.919bp)  .. (d2);
    \draw [->] (s1) ..controls (145.8bp,64.803bp) and (132.68bp,51.685bp)  .. (d4);
    \draw [->] (s4) ..controls (124.2bp,64.803bp) and (137.32bp,51.685bp)  .. (d1);
    \draw [->] (s2) ..controls (243.0bp,64.131bp) and (243.0bp,54.974bp)  .. (d2);
    \draw [->] (s3) ..controls (52.197bp,64.803bp) and (65.315bp,51.685bp)  .. (d4);
    \draw [->] (s4) ..controls (145.22bp,66.889bp) and (184.76bp,47.119bp)  .. (d2);
    \draw [->] (s2) ..controls (180.05bp,69.018bp) and (104.76bp,43.919bp)  .. (d3);
    \draw [->] (s4) ..controls (73.803bp,64.803bp) and (60.685bp,51.685bp)  .. (d3);
    \draw [->] (s3) ..controls (27.0bp,64.131bp) and (27.0bp,54.974bp)  .. (d3);
    \draw [->] (s3) ..controls (73.222bp,66.889bp) and (112.76bp,47.119bp)  .. (d1);
    \draw [->] (s2) ..controls (217.8bp,64.803bp) and (204.68bp,51.685bp)  .. (d1);
    \draw [->] (s1) ..controls (124.78bp,66.889bp) and (85.238bp,47.119bp)  .. (d3);
    \draw [->] (s1) ..controls (196.2bp,64.803bp) and (209.32bp,51.685bp)  .. (d2);
    \draw [->] (s4) ..controls (99.0bp,64.131bp) and (99.0bp,54.974bp)  .. (d4);
  \end{tikzpicture}
  \caption[neural-network]{A simple neural network}
  \label{fig:neural-network}
\end{figure}

\emph{Running} this network requires computing

\[ X * W + b \]

where \( W \in \mathbb{R}^{4 \times 4} \) is a matrix of weights and \(
b \in \mathbb{R}^{4}\) is a vector of biases, while $X$ is a
\emph{batch} of input vectors in \( \mathbb{R}^4 \).
When the input is a flattened image of 2 by 2 pixels this neural
network can be used to classify 4 different classes of images.

\begin{figure}
  \begin{minted}{python}
    >>> import tensorflow as tf
    >>> W = tf.random_normal(shape=(4, 4))
    >>> b = tf.random_normal(shape=(4,))
    >>> X = tf.placeholder(shape=(None, 4), dtype=tf.float32)
    >>> logits = tf.matmul(X, W) + b
    >>> probabilities = tf.nn.softmax(logits)
    >>>
    >>> probabilities
    <tf.Tensor 'Softmax:0' shape=(?, 4) dtype=float32>
  \end{minted}
  \caption{Using the TensorFlow Python API for $X * W + b$}
\end{figure}

Note that the \texttt{tf.nn.softmax} function is applied to the result of
the matrix multiplication. This is done for various reasons: it
introduces a \emph{non-linear factor} in the network computations
allowing it to learn to compute any function -- what's known as the
``Universal approximation theorem''\footnote{Actually, this network is
too simple to fall under the universal approximation theorem. In fact
you need at least one hidden layer. See
https://en.wikipedia.org/wiki/Universal\_approximation\_theorem}; also
it normalizes the results of the neural network in $[0,1]$ allowing to
interpret the output in terms of confidence levels or, as they are
improperly called, \emph{probabilities}.

To get results out of the graph you need what's called a session in
TensorFlow. Basically a session \emph{powers on} the graph, allowing
you to run the computational graph with your own data and get actual
tensors of actual numbers out of it. In Figure {fig:use-session} we use a
session to get probabilities out of our neural network for a batch of
\( 2 \times 2 \) images.

\begin{figure}
  \begin{minted}{python}
    >>> import numpy as np
    >>> ten_two_by_two_images_batch = np.random.rand(10, 4)
    >>> ten_two_by_two_images_batch
    array([[0.54485176, 0.33854871, 0.45185129, 0.79884188],
          [0.41204776, 0.23552753, 0.04101023, 0.47883844],
          [0.25544491, 0.7610509 , 0.49307137, 0.6098213 ],
          [0.02545156, 0.70459456, 0.22067103, 0.64743811],
          [0.92359354, 0.96497353, 0.45790538, 0.49380769],
          [0.13330072, 0.22947966, 0.02996348, 0.69954114],
          [0.38397249, 0.30473362, 0.87023559, 0.90153084],
          [0.77056319, 0.94843128, 0.39095345, 0.50572861],
          [0.90112077, 0.19240995, 0.48437166, 0.46200152],
          [0.98589042, 0.2013479 , 0.86091217, 0.55886214]])
    >>> with tf.Session() as session:
    ...     # we're _feeding_ the X placeholder with an actual numpy array
    ...     session.run(probabilities, feed_dict={X: ten_two_by_two_images_batch})
    ...
    array([[0.03751625, 0.8651346 , 0.05245555, 0.04489357],
          [0.11873736, 0.6301607 , 0.17646323, 0.07463871],
          [0.06252376, 0.82338244, 0.07284217, 0.04125158],
          [0.12086256, 0.6911012 , 0.14362463, 0.04441159],
          [0.03662292, 0.8575108 , 0.04834893, 0.05751737],
          [0.12984137, 0.63064647, 0.1834405 , 0.05607158],
          [0.01711418, 0.93650085, 0.02116078, 0.02522417],
          [0.04886489, 0.83023334, 0.06332602, 0.05757581],
          [0.033136  , 0.84808177, 0.05061754, 0.06816475],
          [0.01250888, 0.9262628 , 0.01797607, 0.0432523 ]], dtype=float32)
    >>> #              ^^^^^^^^^ the last image is of the second class with a confidence of 92%
  \end{minted}
  \caption{}
\end{figure}

Of course building neural networks is pretty useless if you can't train
them. In fact the probabilities above are totally non-sense: they're
only based on the initial weight and bias randomly extracted from a
uniform distribution. There's no training set, no training phase yet. To
train a network in TensorFlow we need a \texttt{tf.Variable}. A
variable in TensorFlow is a tensor whose value persists across sessions
and that can be modified. This allows you to basically add parameters
to your computational graphs, allowing you to compute different
functions with the same graph. That, combined with a learning procedure
that modifies the \texttt{tf.Variable}s allows you to train your model.

To build a \texttt{tf.Variable} in TensorFlow you can just wrap an
existing tensor. In the example above, the part of the graph that you
want to parameterize are the weight matrix $W$ and the bias vector
$b$.

\begin{figure}
  \begin{minted}{python}
    W = tf.Variable(W)
    b = tf.Variable(b)
    logits = tf.matmul(X, W) + b # redefine logits: using different tensors now
    probabilities = tf.nn.softmax(logits)
  \end{minted}
  \caption{}
\end{figure}

To perform training we now need a training set. A training set in this
example woule consist of lots of \(2 \times 2\) images and an ID
associated to each image representing the class of that image.

\emph{Training} basically means minimizing a loss function between the
provided IDs of the training set and the IDs guessed by the neural
network. The loss function we're going to use is called
\emph{cross-entropy} and TensorFlow implements it via
\texttt{tf.nn.softmax\_cross\_entropy\_with\_logits\_v2}

\begin{figure}
  \begin{minted}{python}
    y_true = tf.placeholder(shape=(None,), dtype=tf.float32)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_true)
  \end{minted}
  \caption{}
\end{figure}

To minimize the cross-entropy we're using the Gradient descent
algorithm -- an minimization technique that works iteratively.
TensorFlow implements that algorithm via a class:
\texttt{GradientDescentOptimizer}. Instances of that class expose
a \texttt{.minimize()} method that returns a tensor that each time you
run it in a session will modify the graph's \texttt{tf.Variable}s,
trying to lower the value of the loss function.

\begin{figure}
  \begin{minted}{python}
    from tf.train import GradientDescentOptimizer as SGD
    optimizer = SGD(learning_rate=0.5).minimize(cross_entropy)
  \end{minted}
  \caption{}
\end{figure}

So a typical training phase would consist of

\begin{minted}{python}
  with tf.Session() as session:
      session.run(tf.global_variables_initializer()) # needed.

  n_training_steps = 10
  for i in range(n_training_steps):
      images, classes = next_batch()
      # fictitious function returning batches from the training set

      with tf.Session() as session:
          session.run(optimizer, feed_dict={X: images, y_true: classes})
\end{minted}

As you increase \texttt{n\_training\_steps}, \texttt{W} and \texttt{b}
will be modified pushing the value of the loss function towards 0. That
makes the model learn how to correctly classify images similar to those
in the training set.

\section{Keras}
\label{sec:keras}

Keras is a high level library for building Machine Learning models. It
consists of a simple API and bindings for a backend of choice, most
notably TensorFlow. When you use the library, you're encouraged to use
\emph{layers} instead of plain tensors. In fact the whole concept of
TensorFlow tensors is hidden away by the library abstractions. To
implement your model you stack layers -- basically you build a list of
layers. For example, to implement a simple neural network one would
stack a \texttt{Dense} layer of 4 hidden neurons and an
\texttt{Activation} layer for the Softmax function.

\begin{minted}{python}
  from keras.models import Sequential
  model = Sequential([
    Dense(batch_input_shape=(None, 4), units=4),
    Activation('softmax')
  ])
\end{minted}

Under the hood, a \texttt{Layer} is simply a callable object whose
\texttt{\_\_call()\_\_} method takes a TensorFlow tensor \texttt{X} and
manipulates it. For example a simple custom one would be
\begin{minted}{python}
  from keras.engine.base_layer import Layer
  import tensorflow as tf
  class Add42(Layer):
      def __init__(self):
          self.fortytwo = tf.constant(42)

      def __call__(self, X):
          return tf.add(X, self.fortytwo)
\end{minted}

While the idea is beginner friendly -- as it allows people to
immediately concentrate on model's architecture instead of thinking
about tensors -- at the time of this writing the abstraction can become
leaky when things goes wrong, needing the programmer to know about
TensorFlow computational graphs to debug her program successfully.

\section{CleverHans}
\label{sec:cleverhans}

CleverHans is a library written by Google for building adversarial
examples. It implements a variety of attacks against neural networks
(FGS, T-FGS, Carlini and Wagner, ...) and it's compatible with models
built with Keras or plain TensorFlow. It uses the same computational
graphs of TensorFlow to generate adversarial examples; again, as you
use the library more and more understanding computational graphs
becomes a necessity.

\section{Scikit-learn}
\label{sec:sklearn}

Scikit-learn is a Python library written by Google providing a number
of models, learning algorithms and other utilities for Machine
Learning. It's perfect for fast prototyping as it uses a simple and
consistent API and handles numpy arrays or even Python lists.

I've used scikit-learn to borrow a couple of decomposition algorithms
(PCA, FastICA, ...) without having to implement them. This required a
bit of reasoning as reusing scikit-learn algorithms on TensorFlow
graphs was not straightforward.

\section{Google Cloud Platform}
\label{sec:google-cloud-platform}

Google Cloud Platform is a set of services that allows people to
\emph{borrow} computational power from Google. In our case, we used
Google Cloud Platform to get access to a GPU for few dollars a month.
That made running experiments a lot faster in many cases.
