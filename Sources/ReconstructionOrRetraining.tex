%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Reconstruction or retraining?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As we're testing different filter techniques it's important that we
understand how we're implementing these techniques in our models as
defenses.

\section{Motivation}
\label{sec:motivation}
There are at least two different fashions to integrate a
filter/decomposition technique in our models. We can either add a layer
to a pre-trained model and use it only \emph{after} training, or we can
add a layer to a pre-trained model and re-train the network after this
layer has been added. In the latter case, the model has been trained
only on filtered input, while in the former it does not. Following
lexicon used in ???, we named the first technique as
\emph{reconstruction} and the second one \emph{retraining}.

There's another alternative which is to initialize the model, add the
filter layer then train the network while it's still freshly
initialized. It basically consists in making the network never to see
unfiltered input. This seemed to make things so much better for the
attacker than both reconstruction and retraining that we just avoided
to compare it with the other approaches. Instead when it comes to
reconstruction and retraining who is better is not that clear. We did
some measurements to better understand it.

\section{How did we perform the comparison}

We built and train models using both reconstruction and retraining. We
had now two families of models. For example, for FC-100-100-10 with a
filter layer implementing PCA retaining 80 components we had one model
trained using reconstruction and another trained using retraining.

We attacked each model using Fast Gradient Sign obtaining a curve
saying what was the adversarial success score as $\eta$ increases --
where $\eta$ is the freedom given to the attacker: the more the freedom
the easier is to forge an input. Then, we paired each model trained
using reconstruction with the same model trained using retraining. We
computed the average value of the curve obtained subtracting the curve
for retraining from the curve for reconstruction. At last we computed
the average for the set of pairs, getting what is the average difference
between reconstruction and retraining.

\section{Data obtained}
We obtained that retraining is only 1\% more effective than reconstruction
when it comes to protect models from adversarial input.

\section{Conclusion}
While implementing retraining defense is not hard we concluded the
difference is not significant enough and decided to stick with
reconstruction which is a clearer solution -- it allows us to think about
filter techiniques without worrying about how that impacts on training.
