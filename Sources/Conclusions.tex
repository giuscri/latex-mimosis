%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This thesis was about how dimensionality reduction techniques, apart
from Principal Components Analysis, could help in reducing the rate of
success for an attacker forging malicious input.

We started by introducing some background knowledge on Machine Learning
and the topic of adversarial examples. Then we described the tools we
used at a high level, and we finished with a chapter where the results
of the experiments are finally discussed.

This work took more or less three months, basically a student's summer
break. This has been my first experience with Machine Learning so I had
to spend the required time learning about fundamentals and primitives
of the field: what is the training set? the validation set? cross
validation? what is the gradient descent and what is the learning rate?
what is a neural network? what is backpropagation?

This thesis was an attempt to combine a genuine curiosity about Machine
Learning --- being it a hot topic right now --- with my passion of
watching \emph{computers} to break. I find that the topic of
adversarial examples can be as fascinating to security-oriented
computer scientists just like memory corruption or other traditional
computer security topics are \cite{DBLP:journals/corr/PapernotMSW16}.
The culture and the medium to share knowledge are very different: on
one hand, in the case of Machine Learning, you have mostly books and
research papers; on the other hand, in the case of traditional
security, you have RFCs, tweets from mysterious independent
researchers, pastebins and blog posts. Yet some attempt has been done
to make the two scenes communicate%
\footnote{https://youtu.be/JAGDpJFFM2A, ``Machine Duping 101: Pwning
  Deep Learning Systems''}
\footnote{https://youtu.be/wbRx18VZlYA, ``Weaponizing Machine
  Learning''}.

As explained in Chapter \ref{ch:implementation-of-robust-networks} the
results of this thesis are that it is hard to improve the performance of
a model protected by a Principal Components Analysis filter just by
swapping that dimensionality reduction technique with some other
technique. That is, the original hypothesis is not been confirmed.

Nonetheless, I'm rather satisfied by this work. I did choose it mostly
to get some hands-on experience on Machine Learning. I previously
struggled to find the will to deep dive into such an unknown topic so I
exploited this opportunity --- the thesis for my Bachelor of Science
degree --- to study it, making it something required to know before
graduation, at least at a high level.
