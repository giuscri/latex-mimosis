%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Training Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Before running experiments on how ML models can be \emph{shielded}
against adversarial examples we had to create and train them.

\section{Motivation}
\label{sec:motivation}
As we're training a variety of models it's useful to find a strategy to
find an appropriate number of epochs to train the model for. Fixing a
number of epochs (say 500) and train all the models for that same
number of epochs feels \emph{cheesy}: some models can learn faster,
other slower, and you can end up comparing models that are at different
stage in their learning phase.

What we did was trying to \emph{understand} when the model has done
learning and stop training there. We used two approaches/heuristics
that are described in the following sections.

Note that trying to make the model learn better we reduced the learning
rate on plateau: if for a fixed number of epochs the accuracy on the
validation set does no longer improve, the learning rate is reduced.
This is known to help models to move on when the gradient descent
algorithm is stuck somewhere.

To make our experiments here we used a fully-connected neural network
with 2 layers of 100 hidden neurons each and 10 output neurons --
\emph{FC-100-100-10}. This structure is the same described in ???,
which is the paper we got inspiration from. ??? trained FC-100-100-10
for 500 epochs and we assume they chose that number as it gave them the
best results on the test set.

What we want to do is to empirically prove that we can achieve the
same accuracy on the test set without training FC-100-100-10 for
exactly 500 epochs but instead stopping training when the model stops
learning according to at least one of our heuristics.

\section{EarlyStopping}
\emph{EarlyStopping} is one of the two heuristics. It's already
implemented by Keras as a Callback. It checks if over a specified
number of epochs (the \emph{patience}) a specified metrics stopped
improving. When that happens the model is asked to stop training there.
We set a \emph{patience} of 60 epochs as on Google Cloud Platform 1
epochs runs is ~1 second; a patience of 60 epochs means that the model
didn't improve for a whole minute.

\section{StopOnStableWeights}
\emph{StopOnStableWeights} is the second heuristic. It has to be
implemented from scratch. This heuristic collects data on model's
weights over a specified number of epochs called the \emph{patience}.
Then, the same weight over different epochs is checked: if it has a
small standard deviation maybe it's getting stable. If the maximum of
these standard deviation is below a certain threshold we conclude the
model's weights are getting stable, hence the model is stopping
learning.

\section{Data obtained}
We obtained the following data:
\begin{itemize}
\item
  Model trained for 1000 epochs: accuracy on the test set of 97.71\%
\item
  Model trained for 500 epochs: accuracy on the test set of 97.55\%
\item
  Model trained for 500 epochs, reducing learning rate on plateau after
  30 epochs: accuracy on the test set of 97.47\%
\item
  Model trained for -1 epochs, stopping after accuracy on the
  validation set stopped improving for 60 epochs: accuracy on the test
  set of 97.44\% (trained for 424 epochs)
\item
  Model trained for -1 epochs, stopping as model weights had a maximum
  standard deviation of 0.5 over 60 epochs: accuracy on the test set of
  97.66\% (trained for 614 epochs)
\item
  Model trained for -1 epochs, stopping after accuracy on the
  validation set stopped improving for 60 epochs, reducing learning
  rate on plateau after 30 epochs: accuracy on the test
  set of 97.18\% (trained for 510 epochs)
\item
  Model trained for -1 epochs, stopping as model weights had a maximum
  standard deviation of 0.5 over 60 epochs, reducing learning
  rate on plateau after 30 epochs: accuracy on the test set of
  97.6\% (trained for 514 epochs)
\end{itemize}

\section{Conclusion}
From the above results the best approach seems to be using the
\emph{StopOnStableWeights} heuristics with a patience of 60 epochs,
reducing the learning rate on plateau after 30 epochs.

In the following chapters we'll use this setting for training models,
even when they're no longer the FC-100-100-10 used here.
